import os
import glob
import fitz
import json
import logging
import time
import threading
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, StreamingResponse, Response
from pydantic import BaseModel
from dotenv import load_dotenv
from typing import Optional

# --- LangChain & Agent Imports ---
from langchain_openai import ChatOpenAI
from langchain_classic.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.tools import Tool
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.callbacks import BaseCallbackHandler

# --- RAG Imports ---
from langchain_pinecone import PineconeVectorStore
from langchain_community.embeddings import FastEmbedEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_classic.retrievers.multi_query import MultiQueryRetriever

# --- Supabase & Pinecone ---
from supabase import create_client
from pinecone import Pinecone, ServerlessSpec

from weather_service import WeatherService

load_dotenv()

logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

# --- Globals initialised in lifespan (after port is bound) ---
_supabase = None
_agent_executor = None
_advanced_retriever = None
current_active_date = "2025-01-01"


# --- Supabase helpers ---
def db_get_chat(chat_id: str):
    res = _supabase.table("chats").select("chat_id").eq("chat_id", chat_id).execute()
    return res.data[0] if res.data else None


def db_create_chat(chat_id: str, user_name: str, title: str):
    _supabase.table("chats").insert(
        {"chat_id": chat_id, "user_name": user_name, "title": title}
    ).execute()


def db_get_history(chat_id: str):
    res = _supabase.table("messages").select("role, content").eq("chat_id", chat_id).order("id").execute()
    return res.data


def db_save_messages(chat_id: str, user_msg: str, bot_msg: str):
    _supabase.table("messages").insert([
        {"chat_id": chat_id, "role": "user", "content": user_msg},
        {"chat_id": chat_id, "role": "bot",  "content": bot_msg},
    ]).execute()


def db_get_user_chats(user_name: str):
    res = _supabase.table("chats").select("chat_id, title").eq("user_name", user_name).execute()
    return res.data


def db_delete_chat(chat_id: str):
    _supabase.table("messages").delete().eq("chat_id", chat_id).execute()
    _supabase.table("chats").delete().eq("chat_id", chat_id).execute()


# --- Pinecone / RAG setup ---
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "agri-advisor")


def build_index(embeddings):
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    existing = [idx.name for idx in pc.list_indexes()]

    if PINECONE_INDEX_NAME not in existing:
        print(f"[Pinecone] Creating index '{PINECONE_INDEX_NAME}'...")
        pc.create_index(
            name=PINECONE_INDEX_NAME,
            dimension=384,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
        time.sleep(5)

    index = pc.Index(PINECONE_INDEX_NAME)
    vs = PineconeVectorStore(index=index, embedding=embeddings)

    stats = index.describe_index_stats()
    total_vectors = stats.total_vector_count

    if total_vectors == 0:
        print("[Pinecone] Index empty â€” indexing PDFs...")
        pdf_files = glob.glob("project_sources/*.pdf")
        docs_list = []
        for pdf in pdf_files:
            doc = fitz.open(pdf)
            for page in doc:
                text = page.get_text()
                if text.strip():
                    docs_list.append(Document(
                        page_content=text,
                        metadata={"source": os.path.basename(pdf)}
                    ))
            doc.close()
        if docs_list:
            chunks = RecursiveCharacterTextSplitter(
                chunk_size=1000, chunk_overlap=200
            ).split_documents(docs_list)
            vs.add_documents(chunks)
            print(f"[Pinecone] Indexed {len(chunks)} chunks.")
    else:
        print(f"[Pinecone] Index ready â€” {total_vectors} vectors.")

    return vs


# --- Heavy initialisation (runs in background thread after port is bound) ---
def _init_services():
    global _supabase, _agent_executor, _advanced_retriever
    try:
        print("[Init] Connecting to Supabase...")
        _supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

        print("[Init] Loading LLM...")
        llm = ChatOpenAI(
            api_key=os.getenv("LLMOD_API_KEY"),
            base_url=os.getenv("LLMOD_API_BASE", "https://api.llmod.ai/v1"),
            model="RPRTHPB-gpt-5-mini",
            temperature=1,
            streaming=True
        )

        print("[Init] Loading embeddings model...")
        embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")

        print("[Init] Connecting to Pinecone...")
        vectorstore = build_index(embeddings)

        if vectorstore:
            _advanced_retriever = MultiQueryRetriever.from_llm(
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                llm=llm
            )

        weather_service = WeatherService()

        def search_pdf(query: str):
            if not _advanced_retriever:
                return "Error: No professional documents found in the system."
            print(f"\n[RAG DEBUG] Advanced RAG Search: '{query}'")
            docs = _advanced_retriever.invoke(query)
            if not docs:
                return "NO RESULTS FOUND in the knowledge base. SYSTEM COMMAND: You MUST try calling this tool again using different, broader, or alternative agricultural keywords before answering the user."
            return "\n\n".join([d.page_content for d in docs])

        def weather_tool_wrapper(city_input: str):
            clean_city = str(city_input).replace("on", "").replace(current_active_date, "").strip()
            return weather_service.get_weather(f"{clean_city} on {current_active_date}")

        tools = [
            Tool(name="weather_lookup",      func=weather_tool_wrapper, description="MUST be used for any weather or temperature request."),
            Tool(name="agri_knowledge_base", func=search_pdf,           description="Search agricultural manuals. If it returns NO RESULTS, try again with different words."),
        ]

        prompt = ChatPromptTemplate.from_messages([
            ("system", """××ª×” ××’×¨×•× ×•× ××•××—×” ×•×™×“×™×“×•×ª×™ ×‘×™×©×¨××œ.
    1. ×‘×©×™×—×•×ª ×—×•×œ×™×Ÿ: ×¢× ×” ×‘× ×™××•×¡ ×•××œ ×ª×©×ª××© ×‘×›×œ×™×.
    2. ×‘×©××œ×•×ª ××§×¦×•×¢×™×•×ª: ×—×•×‘×” ×œ×”×©×ª××© ×‘×›×œ×™ 'weather_lookup'.
    3. ×”×”×§×©×¨ ×”× ×¡×ª×¨ ×©××•×¢×‘×¨ ××œ×™×š ××›×™×œ ××ª ×”×ª××¨×™×š ("×”×™×•×") ×•×”××™×§×•×.
    4. ×—×•×‘×” ×œ×”×©×ª××© ×‘×›×œ×™ 'agri_knowledge_base' ×œ×©××œ×•×ª ×¢×œ ×’×™×“×•×œ×™×. ×× ×”×›×œ×™ ××—×–×™×¨ ×©××™×Ÿ ×ª×•×¦××•×ª, ×—×•×‘×” ×¢×œ×™×š ×œ×”×¤×¢×™×œ ××•×ª×• ×©×•×‘ ×¢× ××™×œ×•×ª ××¤×ª×— ××—×¨×•×ª ×œ×¤×—×•×ª ×¤×¢× ××—×ª ×œ×¤× ×™ ×©××ª×” ××ª×™×™××©.
    5. ×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“ ×•×‘×¦×•×¨×” ××§×¦×•×¢×™×ª."""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        _agent_executor = AgentExecutor(
            agent=create_openai_tools_agent(llm, tools, prompt),
            tools=tools,
            verbose=True
        )
        print("[Init] Agent ready âœ“")

    except Exception as e:
        print(f"[Init ERROR] Initialisation failed: {e}")


# --- FastAPI lifespan: binds port immediately, init runs in background ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Kick off heavy init in a daemon thread â€” port binds without waiting
    threading.Thread(target=_init_services, daemon=True).start()
    yield
    # shutdown â€” nothing to clean up


app = FastAPI(lifespan=lifespan)
app.mount("/static", StaticFiles(directory="static"), name="static")


# --- Steps Callback Handler ---
class StepsCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.steps = []
        self._pending_tool = None
        self._pending_llm_prompt = None

    def on_llm_start(self, serialized, messages, **kwargs):
        prompt_msgs = []
        for item in messages:
            msg_list = item if isinstance(item, list) else [item]
            for msg in msg_list:
                if hasattr(msg, "content"):
                    content = msg.content if isinstance(msg.content, str) else str(msg.content)
                    prompt_msgs.append({"role": getattr(msg, "type", "message"), "content": content[:600]})
                else:
                    prompt_msgs.append({"role": "message", "content": str(msg)[:600]})
        self._pending_llm_prompt = prompt_msgs

    def on_llm_end(self, response, **kwargs):
        response_data = {}
        if response.generations:
            gen = response.generations[0][0]
            text = getattr(gen, "text", "")
            if text:
                response_data = {"text": text[:1000]}
            else:
                msg = getattr(gen, "message", None)
                tool_calls = getattr(msg, "tool_calls", []) if msg else []
                if tool_calls:
                    response_data = {"tool_calls": [{"tool": tc.get("name"), "args": tc.get("args")} for tc in tool_calls]}
                else:
                    response_data = {"text": str(gen.text)[:500]}
        self.steps.append({
            "module": "AgentLLM",
            "prompt": {"messages": self._pending_llm_prompt or []},
            "response": response_data
        })

    def on_tool_start(self, serialized, input_str, **kwargs):
        tool_name = serialized.get("name", "unknown")
        module = "WeatherTool" if tool_name == "weather_lookup" else "AgriKnowledgeBase"
        self._pending_tool = {"module": module, "prompt": {"query": str(input_str)[:500]}}

    def on_tool_end(self, output, **kwargs):
        if self._pending_tool:
            self.steps.append({
                "module": self._pending_tool["module"],
                "prompt": self._pending_tool["prompt"],
                "response": {"output": str(output)[:800]}
            })
            self._pending_tool = None


class ExecuteRequest(BaseModel):
    prompt: str
    user_name: Optional[str] = None
    chat_id: Optional[str] = None
    city: Optional[str] = ""
    date: Optional[str] = ""


class ChatRequest(BaseModel):
    user_name: str
    chat_id: str
    query: str
    city: str
    date: str


# --- API Routes ---
@app.get("/")
async def home():
    return FileResponse("static/index.html")


@app.post("/api/execute")
async def execute(req: ExecuteRequest):
    global current_active_date
    if req.date:
        current_active_date = req.date

    if _agent_executor is None or _supabase is None:
        return {"status": "error", "error": "Agent is still initialising, please try again in a moment.", "response": None, "steps": []}

    full_input = req.prompt
    if req.city or req.date:
        full_input = f"[×”×§×©×¨ × ×¡×ª×¨: ×”×ª××¨×™×š ×”×™×•× ×”×•× {req.date}, ×”××™×§×•× ×”×•× {req.city}]\n×”×•×“×¢×ª ×”××©×ª××©: {req.prompt}"

    try:
        history = []
        if req.chat_id:
            if req.user_name and not db_get_chat(req.chat_id):
                title = f"{req.prompt[:15]}... | {req.city} | {req.date}"
                db_create_chat(req.chat_id, req.user_name, title)
            hist_rows = db_get_history(req.chat_id)
            history = [
                HumanMessage(content=r["content"]) if r["role"] == "user" else AIMessage(content=r["content"])
                for r in hist_rows
            ]

        handler = StepsCallbackHandler()
        result = _agent_executor.invoke(
            {"input": full_input, "chat_history": history},
            config={"callbacks": [handler]}
        )
        answer = result.get("output", "")
        if req.chat_id:
            db_save_messages(req.chat_id, req.prompt, answer)
        return {"status": "ok", "error": None, "response": answer, "steps": handler.steps}
    except Exception as e:
        return {"status": "error", "error": str(e), "response": None, "steps": []}


@app.get("/api/model_architecture")
async def model_architecture():
    with open(os.path.join("static", "architecture.png"), "rb") as f:
        data = f.read()
    return Response(content=data, media_type="image/png")


@app.get("/api/agent_info")
async def agent_info():
    return {
        "description": (
            "An autonomous agricultural advisory agent for Israeli farmers. "
            "It combines real meteorological data for 16 Israeli locations with a "
            "professional knowledge base (RAG over agricultural manuals) to deliver "
            "location-aware, date-aware farming advice in Hebrew."
        ),
        "purpose": (
            "Help Israeli farmers make data-driven decisions about irrigation, planting, "
            "pest management and soil health by grounding every answer in actual weather "
            "readings and peer-reviewed agricultural literature."
        ),
        "prompt_template": {
            "template": (
                "Ask any agricultural question. Optionally include a city and date "
                "to get weather-aware advice. Example structure: "
                "'[question] â€“ city: [city], date: [YYYY-MM-DD]'"
            )
        },
        "prompt_examples": [
            {
                "prompt": "××” ×”×˜××¤×¨×˜×•×¨×” ×”×™×•× ×‘×‘××¨ ×©×‘×¢ ×•×”×× ×›×“××™ ×œ×”×©×§×•×ª?",
                "full_response": (
                    "×”×˜××¤×¨×˜×•×¨×” ×”×™×•× ×‘×‘××¨ ×©×‘×¢ ×”×™× 34Â°C ×¢× ×œ×—×•×ª ×©×œ 22%. "
                    "××•××œ×¥ ×œ×”×©×§×•×ª ×‘×©×¢×•×ª ×”×‘×•×§×¨ ×”××•×§×“××•×ª (05:00â€“07:00) ×›×“×™ ×œ××–×¢×¨ ××™×“×•×™. "
                    "×›××•×ª ××™× ××•××œ×¦×ª: 6â€“8 ×\"× ×œ×˜×¤×˜×•×£."
                ),
                "steps": [
                    {"module": "WeatherTool",      "prompt": {"city": "beer sheva", "date": "2025-07-01"}, "response": {"temp": "34Â°C", "humidity": "22%", "rain": "0mm"}},
                    {"module": "AgriKnowledgeBase", "prompt": {"query": "irrigation high temperature dry conditions"}, "response": {"excerpt": "Irrigate in early morning to minimize evaporation..."}},
                    {"module": "AgentLLM",          "prompt": {"context": "weather + rag results"}, "response": {"answer": "×”×©×§×” ×‘×‘×•×§×¨ ××•×§×“×, 6â€“8 ×\"×"}}
                ]
            },
            {
                "prompt": "××ª×™ ×›×“××™ ×œ×–×¨×•×¢ ×—×™×˜×” ×‘× ×™×¦×Ÿ ×”×©× ×”?",
                "full_response": (
                    "×‘× ×™×¦×Ÿ, ×¢×•× ×ª ×”×–×¨×™×¢×” ×”××•×¤×˜×™××œ×™×ª ×œ×—×™×˜×” ×”×™× ×‘×™×Ÿ ××•×§×˜×•×‘×¨ ×œ× ×•×‘××‘×¨. "
                    "×œ×¤×™ ×”× ×ª×•× ×™× ×”××§×œ×™××™×™× ×•×œ×—×•×ª ×”×§×¨×§×¢ ×”×¦×¤×•×™×”, ××•××œ×¥ ×œ×–×¨×•×¢ ×‘×¡×•×£ ××•×§×˜×•×‘×¨."
                ),
                "steps": [
                    {"module": "AgriKnowledgeBase", "prompt": {"query": "wheat sowing season Israel"}, "response": {"excerpt": "Optimal wheat sowing in Israel: Octoberâ€“November..."}},
                    {"module": "WeatherTool",        "prompt": {"city": "nitzan", "date": "2025-10-15"}, "response": {"temp": "24Â°C", "humidity": "55%", "rain": "3mm"}},
                    {"module": "AgentLLM",           "prompt": {"context": "rag + weather"}, "response": {"answer": "×–×¨×¢ ×‘×¡×•×£ ××•×§×˜×•×‘×¨"}}
                ]
            }
        ]
    }


@app.get("/api/team_info")
async def team_info():
    return {
        "group_batch_order_number": "BATCH_ORDER",
        "team_name": "Team Name",
        "students": [
            {"name": "Student A", "email": "a@example.com"},
            {"name": "Student B", "email": "b@example.com"},
            {"name": "Student C", "email": "c@example.com"}
        ]
    }


@app.post("/get-advice")
async def get_advice(req: ChatRequest):
    global current_active_date
    current_active_date = req.date

    if not db_get_chat(req.chat_id):
        title = f"{req.query[:15]}... | {req.city} | {req.date}"
        db_create_chat(req.chat_id, req.user_name, title)

    hist_rows = db_get_history(req.chat_id)
    history = [
        HumanMessage(content=r["content"]) if r["role"] == "user" else AIMessage(content=r["content"])
        for r in hist_rows
    ]

    async def event_generator():
        full_input = f"[×”×§×©×¨ × ×¡×ª×¨: ×”×ª××¨×™×š ×”×™×•× ×”×•× {req.date}, ×”××™×§×•× ×”×•× {req.city}]\n×”×•×“×¢×ª ×”××©×ª××©: {req.query}"
        final_answer = ""
        try:
            yield f"data: {json.dumps({'type': 'status', 'message': 'ğŸ¤” ×× ×ª×— ××ª ×”×‘×§×©×”...'})}\n\n"
            async for event in _agent_executor.astream_events(
                {"input": full_input, "chat_history": history}, version="v2"
            ):
                kind = event["event"]
                if kind == "on_tool_start":
                    if event["name"] == "weather_lookup":
                        yield f"data: {json.dumps({'type': 'status', 'message': 'ğŸŒ¤ï¸ ×©×•×œ×£ × ×ª×•× ×™ ××§×œ×™× ×œ××™×§×•× ×–×”...'})}\n\n"
                    elif event["name"] == "agri_knowledge_base":
                        yield f"data: {json.dumps({'type': 'status', 'message': 'ğŸ“š ××—×¤×© ×™×“×¢ ×‘×—×§×œ××•×ª ×—×›××”...'})}\n\n"
                elif kind == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    if hasattr(chunk, "content") and isinstance(chunk.content, str) and chunk.content:
                        final_answer += chunk.content
                        yield f"data: {json.dumps({'type': 'token', 'text': chunk.content})}\n\n"
            db_save_messages(req.chat_id, req.query, final_answer)
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        except Exception as e:
            print(f"Streaming Error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': '×©×’×™××” ×‘×—×™×‘×•×¨ ×œ××•×“×œ ×”××—×©×‘×”.'})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@app.get("/api/my-chats/{user_name}")
async def get_chats(user_name: str):
    return db_get_user_chats(user_name)


@app.get("/api/chat-history/{chat_id}")
async def get_hist(chat_id: str):
    return db_get_history(chat_id)


@app.delete("/api/delete-chat/{chat_id}")
async def del_chat(chat_id: str):
    db_delete_chat(chat_id)
    return {"status": "ok"}


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
